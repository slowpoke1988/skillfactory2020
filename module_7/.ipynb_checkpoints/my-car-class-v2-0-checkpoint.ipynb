{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Классификация изображений\n",
    "\n",
    "### Основная идея этого решения: взять предобученую на ImageNet сеть EfficientNetB5 и дообучить под нашу задачу. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install - q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import zipfile\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as C\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter\n",
    "# увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "# графики в svg выглядят более четкими\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "print('Keras        :', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Работаем с Tensorflow v2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В setup выносим основные настройки: так удобнее их перебирать в дальнейшем.\n",
    "\n",
    "EPOCHS = 5  # эпох на обучение\n",
    "BATCH_SIZE = 8  # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR = 1e-3\n",
    "VAL_SPLIT = 0.15  # сколько данных выделяем на тест = 15%\n",
    "\n",
    "CLASS_NUM = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE = 224  # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS = 3   # у RGB 3 канала\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "PATH = \"../working/car/\"  # рабочая директория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Устаналиваем конкретное значение random seed для воспроизводимости\n",
    "# os.makedirs(PATH,exist_ok=False)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Category.value_counts()\n",
    "# распределение классов достаточно равномерное - это хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Распаковываем картинки')\n",
    "# Will unzip the files so that you can see them..\n",
    "for data_zip in ['train.zip', 'test.zip']:\n",
    "    with zipfile.ZipFile(\"../input/\"+data_zip, \"r\") as z:\n",
    "        z.extractall(PATH)\n",
    "\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print('Пример картинок (random sample)')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "random_image = train_df.sample(n=9)\n",
    "random_image_paths = random_image['Id'].values\n",
    "random_image_cat = random_image['Category'].values\n",
    "\n",
    "for index, path in enumerate(random_image_paths):\n",
    "    im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n",
    "    plt.subplot(3, 3, index+1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Class: '+str(random_image_cat[index]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры картинок и их размеры чтоб понимать как их лучше обработать и сжимать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(PATH+'/train/0/100380.jpg')\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    #     zoom_range=[0.75,1.25],\n",
    "    validation_split=VAL_SPLIT,  # set validation split\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # set as validation data\n",
    "\n",
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "\n",
    "def imshow(image_RGB):\n",
    "    io.imshow(image_RGB)\n",
    "    io.show()\n",
    "\n",
    "\n",
    "x, y = train_generator.next()\n",
    "print('Пример картинок из train_generator')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    image = x[i]\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(image)\n",
    "    #plt.title('Class: '+str(y[i]))\n",
    "    # plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем предобученную сеть EfficientNetB5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efn.EfficientNetB5(weights='imagenet',\n",
    "                                include_top=False,\n",
    "                                input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D(),)\n",
    "model.add(L.Dropout(rate=0.15))\n",
    "model.add(L.Dense(256, activation='elu'))\n",
    "model.add(L.BatchNormalization())\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# Рекомендация: Попробуйте добавить Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим ModelCheckpoint чтоб сохранять прогресс обучения модели и можно было потом подгрузить и дообучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best_model.hdf5', monitor=[\n",
    "                             'val_accuracy'], verbose=1, mode='max')\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                          patience=4, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=8,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n",
    "# model.save('../working/model_step1.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(\n",
    "    test_generator, steps=len(test_generator), verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning с Fine-tuning\n",
    "**разморозка половины слоев базовой моделив**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers)//2\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../working/model_step2.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineTuning - разморозка всех слоев базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # set as validation data\n",
    "\n",
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.00001\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../working/model_step3.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Увеличим размер изображения и понизим уровень аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-4\n",
    "EPOCHS = 4  # эпох на обучение\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    #     rotation_range = 20,\n",
    "    #     width_shift_range=0.1,\n",
    "    #     height_shift_range=0.1,\n",
    "    #     brightness_range=[0.5, 1.5],\n",
    "    #     zoom_range=[0.75,1.25],\n",
    "    validation_split=VAL_SPLIT,  # set validation split\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efn.EfficientNetB5(weights='imagenet',\n",
    "                                include_top=False,\n",
    "                                input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "model.load_weights('best_model.hdf5')  # Подгружаем ранее обученные веса\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Обучаем\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../working/model_step4.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.reset()\n",
    "predictions = model.predict_generator(\n",
    "    test_sub_generator, steps=len(test_sub_generator), verbose=1)\n",
    "predictions = np.argmax(predictions, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Save submit')\n",
    "\n",
    "# Рекомендация: попробуйте добавить Test Time Augmentation (TTA)\n",
    "# https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d\n",
    "\n",
    "Аугментируем тестовые изображения и сделаем несколько предсказаний одной картинки в разном виде. Взяв среднее значение из нескольких предсказаний получим итоговое предсказание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    #     zoom_range=[0.75,1.25],\n",
    "    validation_split=VAL_SPLIT,  # set validation split\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_steps = 10  # берем среднее из 10 предсказаний\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    preds = model.predict_generator(\n",
    "        test_sub_generator, steps=len(test_sub_generator), verbose=1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(pred, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission_TTA.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Clean PATH\n",
    "import shutil\n",
    "shutil.rmtree(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**В проекте применил методы:**\n",
    "\n",
    "\n",
    "* transfer learning и fine-tuning\n",
    "* настройка LR, optimizer\n",
    "* подобраны переменные (размер картинки, батч и т.д.)\n",
    "* SOTA архитектура сетей - EfficientNetB7\n",
    "* добавлена Batch Normalization и изменена архитектура “головы”\n",
    "* применены дополнительные функции callback Keras https://keras.io/callbacks/\n",
    "* TTA (Test Time Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
